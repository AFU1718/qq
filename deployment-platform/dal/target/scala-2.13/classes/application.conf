akka {
  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
}

akka.system.name = "ady-dal-dev-1"

akka.http.server.preview.enable-http2 = on

app {
  timezone = "Asia/Shanghai"
}

# http {
#   main {
#     host = "0.0.0.0"
#     port = 3366
#   }
#   akka {
#     host = "0.0.0.0"
#     port = 3377
#   }
# }

database {
  main {
    dataSourceClass = org.postgresql.ds.PGSimpleDataSource
    properties = {
      serverName = "adx66"
      portNumber = 5432
      databaseName = "ady"
      user = "root"
      password = "aszx"
    }
    poolName = "dsp_rw"
    connectionTimeout = 500
    maximumPoolSize = 8
    numThreads = 8
  }
  ro = ${database.main} // read only, not used yet
  ro.poolName = "adx_ro"
  report = ${database.main}
  report.properties.databaseName = "ady_report"
  report.properties.user = "root"
  report.poolName = "simple-report"

  file = ${database.main}
  file.properties.databaseName = "file_meta"
  ro.poolName = "file-loading"
}

auth {
  stateless {
    jwt {
      secret = "26uKTlqR8BKpjdJ35DmuN+1q5cA28ep2xKLjR9n2QhI="
      tsec {
        header-name = "X-TSec-JWT"
        expiry-duration = 30.days
        max-idle = 1.days
      }
    }
  }
}


# akka.http {
#   server {
#     # idle-timeout = 300001s
#     request-timeout = 10s
#     registration-timeout = 30s
#     remote-address-header = on
#     parsing.illegal-header-warnings = off
#   }
#   host-connection-pool {
#     max-connections = 8192
#     max-open-requests = 32768
#     max-retries = 5
#     idle-timeout = 30s # default
#     client = {
#       connecting-timeout = 500.milliseconds
#       idle-timeout = 60s # default
#     }
#   }
#   client {
#     connecting-timeout = 500.milliseconds
#     idle-timeout = 60 s # default
#     parsing.illegal-header-warnings = off
#   }
# }

# aerospike {
#   cluster {
#     main {
#       hosts = ["adx61:3000", "adx62:3000"]
#       timeout = 100.milliseconds
#     }
#   }
# }


# cassandra {
#   tables {
#     core-table-name = core
#     event-table-name = event
#     beacon-table-name = bc
#   }
#   common {
#     keyspace = ybc
#     preparedStatementCacheSize = 1000
#     session {
#       contactPoints=["adx66", "adx67", "adx68"]
#       withPort = 9042
#       withoutMetrics = true
#       withoutJMXReporting = false
#       maxSchemaAgreementWaitSeconds = 1
#       addressTranslator = com.datastax.driver.core.policies.IdentityTranslator
#     }
#   }
#   main.write = ${cassandra.common}
#   main.write.session.queryOptions.consistencyLevel=QUORUM // QUORUM // ANY // LOCAL_QUORUM //ONE //LOCAL_QUORUM
#   main.read = ${cassandra.common}
#   main.read.session.queryOptions.consistencyLevel=QUORUM // QUORUM // ANY // LOCAL_QUORUM //ONE //LOCAL_QUORUM
# }

# adx.metrics-expire = 1.hours

# akka.kafka.producer {
#   # Tuning parameter of how many sends that can run in parallel.
#   parallelism = 10000
#   kafka-clients {
#     bootstrap.servers = "192.168.0.61:9092"
#     #  bootstrap.servers = "127.0.0.1:9092"
#     acks = "0"
#     retries = 5
#     batch.size = 16384
#     linger.ms = 1
#     buffer.memory = 33554432
#   }
# }

# pulsar {
#   broker = "pulsar://adx61:6650,adx62:6650,adx63:6650"
#   producer {
#     name = "producer1"
#     persistent = on
#     tenant = "ady"
#     namespace = "dev"
#     topic = "c11"
#   }
#   consumer = ${pulsar.producer}
#   consumer {
#     name = "consumer1"
#     subscription = "s3"
#     subscription-type = "Shared"
#   }
# }
